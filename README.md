# Comparación Entre Las Funciones De Activación ReLU y Leaky ReLU

En este repositorio se pueden encontrar los notebooks correspondientes al reporte tecnico presentando al profesor Carlos Fajardo para la materia de Introducción al Deep Learning

Se encuuentran en toral 6 python notebooks correspondientes a cada uno de los entrenamientos realizados.

2 Entrenamientos con función de activación ReLU 
2 Entrenamientos con función de activación Leaky ReLU con pentiente de 0.01
2 Entrenamientos con función de activación Leaky ReLU con pentiente de 0.2

Si se quieren replicar los experimentos realizados basta con ejecutar nuevamente cada uno de los notebooks

# Recomendaciones para trabajos Futuros

- Se puede jugar con otra arquitectura de red que sea mas adecuada para el problema
- La forma de presentar los datos a la red, podria realizarse primero una conversion de cada imagen a escala de grises y luego de esto si ser presentada como un vetor normalizado, esto con el fin de reducir la cantidad de neuroas en la capa de entrada.
- Hacer uso de tecnias de regularización que permitan mejorar los resultados obtenidos.
